{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG のための前処理\n",
    "\n",
    "指定ディレクトリ内のすべての .txt ファイルを読み込み、NLTK を使ってテキストを文ごとに分割し、指定した文数ごとにチャンク化した上で、各チャンクの埋め込みを生成して出力します。\n",
    "\n",
    "split_text_into_chunks 関数\n",
    "NLTK の sent_tokenize を使用してテキストを文単位に分割し、max_sentences 毎に結合してチャンク化しています。\n",
    "\n",
    "get_embedding 関数\n",
    "最新の OpenAI API では、input パラメータにテキストのリストを渡す必要があるため、input=[text] としています。\n",
    "生成された応答から、最初のチャンクの埋め込みを取り出して返します。\n",
    "\n",
    "main 関数\n",
    "指定ディレクトリ内の全 .txt ファイルを読み込み、各ファイルのテキストをチャンクに分割し、各チャンクに対して埋め込みを生成、最初の5次元だけを出力しています。\n",
    "\n",
    "このコードを実行すれば、まずは小さなサンプルテキストを使った前処理と埋め込み生成の流れを体験できます。\n",
    "その後、生成した埋め込みを Pinecone や Weaviate などのベクトルデータベースに保存するなど、RAG システムの次のステップに進むことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストの分割・前処理\n",
    "- 長い文章は、意味のまとまりごとに分割する必要があります。\n",
    "\n",
    "- たとえば、「文章を文単位に分割し、さらにいくつかの文ごとにまとめてチャンクを作る」方法が考えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 これまでのステップ：RAG構築の準備プロセス\n",
    "### 1. Pineconeのセットアップ\n",
    "- PineconeのWebコンソールにログインし、新しいプロジェクト・インデックス（hamster-embeddings）を作成。\n",
    "\n",
    "- 設定内容：\n",
    "    - Dimension: 1536（OpenAIのtext-embedding-ada-002に対応）\n",
    "    - Metric: cosine\n",
    "    - Type: Dense\n",
    "    - Capacity Mode: Serverless\n",
    "    - Cloud: AWS、Region: us-east-1\n",
    "\n",
    "### 2. Docker環境の構築\n",
    "- Jupyter環境でNotebookを使ってRAG処理を行うため、DockerでPython+Jupyterの環境を構築。\n",
    "\n",
    "- requirements.txtに以下の主要ライブラリを指定：\n",
    "    - openai>=1.0.0\n",
    "    - pinecone-client==2.2.4\n",
    "    - httpx\n",
    "    - nltk\n",
    "    - jupyter\n",
    "\n",
    "- Dockerfileでは追加で build-essential, ca-certificates, curl などもインストールし、通信や証明書の問題を回避。\n",
    "\n",
    "### 3. テキストデータの分割とベクトル化\n",
    "- senarios/ ディレクトリ内の .txt ファイルを読み込み、NLTKの sent_tokenize を使って文単位でチャンクに分割。\n",
    "\n",
    "- OpenAIの text-embedding-ada-002 モデルを使って、各チャンクの文章を1536次元の埋め込みベクトルに変換。\n",
    "\n",
    "### 4. Pineconeへのアップロード（Upsert）\n",
    "- pinecone.Index.upsert() により、ベクトルとメタデータ（元のテキスト）をPineconeに保存。\n",
    "\n",
    "- 実行成功後、Pineconeのダッシュボードで Record Count: 11 を確認（→成功！🎉）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 解決結果: 34.230.203.60\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "try:\n",
    "    print(\"🔍 解決結果:\", socket.gethostbyname(\"hamster-embeddings-4ak3maz.svc.us-east-1-aws.pinecone.io\"))\n",
    "except Exception as e:\n",
    "    print(\"❌ 名前解決できませんでした:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, http://127.0.0.1:8888/tree\n"
     ]
    }
   ],
   "source": [
    "print('hello, http://127.0.0.1:8888/tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI クライアントの初期化\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Pinecone クライアントの初期化\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# インデックスに接続\n",
    "index = pc.Index(\"hamster-embeddings\")\n",
    "\n",
    "# コンテナのディレクトリ\n",
    "directory = \"./senarios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ユーティリティ関数 ---\n",
    "def split_text_into_chunks(text, max_sentences=2):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [\" \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
    "\n",
    "def get_embedding(text):\n",
    "    res = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    return res.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャンクと埋め込みの生成（これで chunks と embeddings が作られます）\n",
    "chunks = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        chunks.extend(split_text_into_chunks(text))\n",
    "\n",
    "# 埋め込みの生成\n",
    "embeddings = [get_embedding(chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings uploaded successfully to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# Pinecone へアップロード（今のエラーが出たセル）\n",
    "\n",
    "vectors = []\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    vector_id = f\"chunk{i+1}\"\n",
    "    metadata = {\"text\": chunks[i]}\n",
    "    vectors.append((vector_id, embedding, metadata))\n",
    "\n",
    "# ベクトルを Pinecone にアップロード\n",
    "index.upsert(vectors=vectors)\n",
    "\n",
    "print(\"✅ Embeddings uploaded successfully to Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 次に目指すステップ\n",
    "以下のような処理を実装していくことで、RAGの完全動作へと進化できます。\n",
    "\n",
    "## ✅ 今後のステップ案\n",
    "### 1.検索（ベクトル検索）機能の実装\n",
    "\n",
    "- クエリテキストを入力し、その埋め込みベクトルを生成。\n",
    "- Pineconeで類似ベクトルを検索し、最も近いテキストを取得。\n",
    "\n",
    "### 2.OpenAIへのプロンプト生成\n",
    "\n",
    "- 検索結果から得たチャンクを組み合わせて「コンテキスト付きプロンプト」を構築。\n",
    "- ChatGPTにそのプロンプトを送信し、回答を得る。\n",
    "\n",
    "### 3.ユーザーインターフェース（例：Streamlit or Flask）\n",
    "- ユーザーがクエリを入力し、検索〜応答生成を体験できるUIを構築。\n",
    "\n",
    "### 4.チャットの履歴保存や拡張（任意）\n",
    "- 会話履歴を保存、マルチターンの対話なども視野に。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
