{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG のための前処理\n",
    "\n",
    "指定ディレクトリ内のすべての .txt ファイルを読み込み、NLTK を使ってテキストを文ごとに分割し、指定した文数ごとにチャンク化した上で、各チャンクの埋め込みを生成して出力します。\n",
    "\n",
    "split_text_into_chunks 関数\n",
    "NLTK の sent_tokenize を使用してテキストを文単位に分割し、max_sentences 毎に結合してチャンク化しています。\n",
    "\n",
    "get_embedding 関数\n",
    "最新の OpenAI API では、input パラメータにテキストのリストを渡す必要があるため、input=[text] としています。\n",
    "生成された応答から、最初のチャンクの埋め込みを取り出して返します。\n",
    "\n",
    "main 関数\n",
    "指定ディレクトリ内の全 .txt ファイルを読み込み、各ファイルのテキストをチャンクに分割し、各チャンクに対して埋め込みを生成、最初の5次元だけを出力しています。\n",
    "\n",
    "このコードを実行すれば、まずは小さなサンプルテキストを使った前処理と埋め込み生成の流れを体験できます。\n",
    "その後、生成した埋め込みを Pinecone や Weaviate などのベクトルデータベースに保存するなど、RAG システムの次のステップに進むことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストの分割・前処理\n",
    "- 長い文章は、意味のまとまりごとに分割する必要があります。\n",
    "\n",
    "- たとえば、「文章を文単位に分割し、さらにいくつかの文ごとにまとめてチャンクを作る」方法が考えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 これまでのステップ：RAG構築の準備プロセス\n",
    "### 1. Pineconeのセットアップ\n",
    "- PineconeのWebコンソールにログインし、新しいプロジェクト・インデックス（hamster-embeddings）を作成。\n",
    "\n",
    "- 設定内容：\n",
    "    - Dimension: 1536（OpenAIのtext-embedding-ada-002に対応）\n",
    "    - Metric: cosine\n",
    "    - Type: Dense\n",
    "    - Capacity Mode: Serverless\n",
    "    - Cloud: AWS、Region: us-east-1\n",
    "\n",
    "### 2. Docker環境の構築\n",
    "- Jupyter環境でNotebookを使ってRAG処理を行うため、DockerでPython+Jupyterの環境を構築。\n",
    "\n",
    "- requirements.txtに以下の主要ライブラリを指定：\n",
    "    - openai>=1.0.0\n",
    "    - pinecone-client==2.2.4\n",
    "    - httpx\n",
    "    - nltk\n",
    "    - jupyter\n",
    "\n",
    "- Dockerfileでは追加で build-essential, ca-certificates, curl などもインストールし、通信や証明書の問題を回避。\n",
    "\n",
    "### 3. テキストデータの分割とベクトル化\n",
    "- senarios/ ディレクトリ内の .txt ファイルを読み込み、NLTKの sent_tokenize を使って文単位でチャンクに分割。\n",
    "\n",
    "- OpenAIの text-embedding-ada-002 モデルを使って、各チャンクの文章を1536次元の埋め込みベクトルに変換。\n",
    "\n",
    "### 4. Pineconeへのアップロード（Upsert）\n",
    "- pinecone.Index.upsert() により、ベクトルとメタデータ（元のテキスト）をPineconeに保存。\n",
    "\n",
    "- 実行成功後、Pineconeのダッシュボードで Record Count: 11 を確認（→成功！🎉）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerコンテナ起動方法\n",
    "- Docker Image を構築する。\n",
    "    - ❯ docker build -t hamster-chunk-text .\n",
    "- Docker Container を使う\n",
    "    - ❯ docker run -it -p 8888:8888 --name hamster-chunk-text \\\n",
    "  --env-file .env \\\n",
    "  -v $(pwd)/senarios:/app/senarios \\\n",
    "  hamster-chunk-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSCodeの中で、コンテナを実行する（カーネルに指定する）方法\n",
    "- ターミナル上でDocker Container をrunしたあとに出てくる以下のログをコピーしておく。\n",
    "    - [I 2025-05-02 08:59:45.108 ServerApp] http://127.0.0.1:8888/tree\n",
    "- VSCode で、この .ipynb を開き、\n",
    "\n",
    "    Notebook 上部に表示される カーネル選択バーで「既存のJupyterサーバーに接続」を選択し、\n",
    "\n",
    "    先ほどコピーした http://127.0.0.1:8888/... を貼り付ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 解決結果: 52.6.114.50\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "try:\n",
    "    print(\"🔍 解決結果:\", socket.gethostbyname(\"hamster-embeddings-4ak3maz.svc.us-east-1-aws.pinecone.io\"))\n",
    "except Exception as e:\n",
    "    print(\"❌ 名前解決できませんでした:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, http://127.0.0.1:8888/tree\n"
     ]
    }
   ],
   "source": [
    "print('hello, http://127.0.0.1:8888/tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI クライアントの初期化\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Pinecone クライアントの初期化\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# インデックスに接続\n",
    "index = pc.Index(\"hamster-embeddings\")\n",
    "\n",
    "# コンテナのディレクトリ\n",
    "directory = \"./senarios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ユーティリティ関数 ---\n",
    "def split_text_into_chunks(text, max_sentences=2):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [\" \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
    "\n",
    "def get_embedding(text):\n",
    "    res = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    return res.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャンクと埋め込みの生成（これで chunks と embeddings が作られます）\n",
    "chunks = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        chunks.extend(split_text_into_chunks(text))\n",
    "\n",
    "# 埋め込みの生成\n",
    "embeddings = [get_embedding(chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings uploaded successfully to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# Pinecone へアップロード（今のエラーが出たセル）\n",
    "\n",
    "vectors = []\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    vector_id = f\"chunk{i+1}\"\n",
    "    metadata = {\"text\": chunks[i]}\n",
    "    vectors.append((vector_id, embedding, metadata))\n",
    "\n",
    "# ベクトルを Pinecone にアップロード\n",
    "index.upsert(vectors=vectors)\n",
    "\n",
    "print(\"✅ Embeddings uploaded successfully to Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 次に目指すステップ\n",
    "以下のような処理を実装していくことで、RAGの完全動作へと進化できます。\n",
    "\n",
    "## ✅ 今後のステップ案\n",
    "### 1.検索（ベクトル検索）機能の実装\n",
    "\n",
    "- クエリテキストを入力し、その埋め込みベクトルを生成。\n",
    "- Pineconeで類似ベクトルを検索し、最も近いテキストを取得。\n",
    "\n",
    "### 2.OpenAIへのプロンプト生成\n",
    "\n",
    "- 検索結果から得たチャンクを組み合わせて「コンテキスト付きプロンプト」を構築。\n",
    "- ChatGPTにそのプロンプトを送信し、回答を得る。\n",
    "\n",
    "### 3.ユーザーインターフェース（例：Streamlit or Flask）\n",
    "- ユーザーがクエリを入力し、検索〜応答生成を体験できるUIを構築。\n",
    "\n",
    "### 4.チャットの履歴保存や拡張（任意）\n",
    "- 会話履歴を保存、マルチターンの対話なども視野に。\n",
    "\n",
    "\n",
    "素晴らしい進捗です🎉  \n",
    "GitHubへのプッシュ完了＆Docker環境構築もバッチリ整って、**RAG（Retrieval-Augmented Generation）基盤の準備完了！**  \n",
    "では、いよいよ **次のステップ「RAGの完成形に向けた構築フェーズ」** に進みましょう！\n",
    "\n",
    "---\n",
    "\n",
    "## 🔜 次のステップ：RAGの「検索→生成」パイプラインの実装\n",
    "\n",
    "以下のような構成に段階的に仕上げていきます：\n",
    "\n",
    "### 🧠 1. クエリ → 埋め込み生成\n",
    "- ユーザーの質問（例：`「ハムスターが夜にうるさい時の対処法」`）を `text-embedding-ada-002` でベクトル化\n",
    "\n",
    "### 📚 2. Pinecone で類似検索\n",
    "- Pinecone に保存済みのチャンクベクトル（`senarios/*.txt`由来）と照合して、類似度の高いチャンクを複数件取得（例：Top5）\n",
    "\n",
    "### 🧾 3. コンテキスト付きでChatGPTへプロンプト送信\n",
    "- 検索結果を `context` として埋め込んだうえで ChatGPT API に問い合わせ\n",
    "- プロンプトテンプレート例：\n",
    "  ```\n",
    "  あなたはハムスター飼育の専門家です。\n",
    "  以下の情報を参考にして質問に答えてください。\n",
    "\n",
    "  コンテキスト:\n",
    "  {context}\n",
    "\n",
    "  質問:\n",
    "  {question}\n",
    "  ```\n",
    "\n",
    "### 💬 4. ChatGPTからの自然な回答をユーザーに提示\n",
    "- 最終的にはこれを**WebアプリやチャットUIに組み込む**ところまでを視野に\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 最初にやること\n",
    "\n",
    "次のNotebookかスクリプトを追加して、検索→生成の流れを実装しましょう。以下のファイル名を提案します：\n",
    "\n",
    "```bash\n",
    "rag_query.ipynb\n",
    "```\n",
    "\n",
    "この中に、以下の処理を入れていきます：\n",
    "\n",
    "### 🎯 rag_query.ipynb の構成案\n",
    "\n",
    "```python\n",
    "# 1. ライブラリ読み込み\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# 2. 初期設定\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=\"us-east-1-aws\")\n",
    "index = pinecone.Index(\"hamster-embeddings\")\n",
    "\n",
    "# 3. 質問入力\n",
    "question = \"ハムスターが夜にうるさい時の対処法は？\"\n",
    "\n",
    "# 4. 質問をベクトル化\n",
    "embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    input=question\n",
    ").data[0].embedding\n",
    "\n",
    "# 5. Pinecone検索\n",
    "response = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "contexts = [match[\"metadata\"][\"text\"] for match in response[\"matches\"]]\n",
    "\n",
    "# 6. プロンプト作成\n",
    "prompt = f\"\"\"あなたはハムスター飼育の専門家です。\n",
    "以下の情報を参考にして質問に答えてください。\n",
    "\n",
    "コンテキスト:\n",
    "{chr(10).join(contexts)}\n",
    "\n",
    "質問:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 7. GPT呼び出し\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "# 8. 出力\n",
    "print(chat_response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 お願い\n",
    "\n",
    "- このコードを `.ipynb` で試したいですか？それとも `.py` スクリプトとして動かしたいですか？\n",
    "- また、**質問はユーザー入力にしたい？**（`input()`やWebUIを使う）かも教えてください！\n",
    "\n",
    "---\n",
    "\n",
    "一緒に「ハムスターチャットボット」完成を目指しましょう〜🐹✨  \n",
    "ご希望の実装スタイルを教えてくれれば、次のファイルを一緒に作ります！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
